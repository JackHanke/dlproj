training:
  learning_rate: 
    initial: 0.2
  batch_size: 512
  momentum: 0.9
  weight_decay: 0.0001
  loss_weights:
    policy_loss: 1.0
    value_loss: 1.0
  training_steps: 700000
  num_self_play_games: 20
  optimizer: "adam"
  data_buffer_size: 50000
  checkpoint_interval: 1000
  evaluation_games: 20
  max_moves: 300

network:
  num_residual_blocks: 13
  num_filters: 256

self_play:
  num_simulations: 500
  temperature:
    initial_moves: 30
  resign_threshold: -0.9
  disable_resignation_fraction: 0.1

evaluation:
  tournament_games: 20
  time_per_move: 5  # seconds
  evaluation_threshold: 0.55
